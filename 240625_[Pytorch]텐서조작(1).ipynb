{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8kw1DLRDLQZ3WB9ErbU0V"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Lzu3FpWB9lUf",
        "outputId": "efc9ed42-8c07-4f62-9f3d-857880bea8db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m725.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.43.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.9)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1 # PyTorch 를 가장 최근 버전으로 설치"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # PyTorch 불러오기\n",
        "import numpy as np # numpy 불러오기\n",
        "import warnings # 경고 문구 제거\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "_qMwvERy-6i-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.텐서이해하기"
      ],
      "metadata": {
        "id": "_LwAXMwR9wjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. 텐서를 생성하고 텐서로 변환하는 방법을 이해 및 실습"
      ],
      "metadata": {
        "id": "UzwjdPTA-sOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텐서 값을 무작위로 생성하는 방법들"
      ],
      "metadata": {
        "id": "ISjr49elAwfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [rand]\n",
        "https://pytorch.org/docs/stable/generated/torch.rand.html"
      ],
      "metadata": {
        "id": "2MFGQbiC_AKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0부터 1 사이의 값을 랜덤하게 NxM 텐서로 반환\n",
        "torch.rand(2, 3) # torch.rand(NxM) NxM은 텐서의 크기를 말합니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVDnBRO5_Fnk",
        "outputId": "3336fc5d-9d53-4aba-f427-699f536914f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7980, 0.7759, 0.7131],\n",
              "        [0.4012, 0.1077, 0.1882]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [randn]\n",
        "https://pytorch.org/docs/stable/generated/torch.randn.html"
      ],
      "metadata": {
        "id": "qHRZzBkS_djq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 가우시안 분포에서 렌덤하게 값을 추출 후, NxM 텐서로 반환\n",
        "torch.randn(2, 3) # torch.randn(NxM) NxM은 텐서의 크기를 말합니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDDY1Ub__erO",
        "outputId": "1ee3e600-171c-476d-9c91-91c0286a5d22"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.1944,  0.1934, -0.7710],\n",
              "        [ 1.0078, -0.6138, -0.0222]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [randint]\n",
        "https://pytorch.org/docs/stable/generated/torch.randint.html"
      ],
      "metadata": {
        "id": "3w-nVXzn_tSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 범위 내의 정수를 N x M 텐서로 반환\n",
        "torch.randint(1, 10, (5, 5)) # 생성 가능한 최솟값 : 1, 최댓값 : 9, (5x5) Tensor 크기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKJlg8JU_vPr",
        "outputId": "c5ec918c-0583-4609-d5b5-9c31da4b026c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3, 5, 9, 4, 2],\n",
              "        [7, 8, 2, 2, 5],\n",
              "        [6, 9, 1, 4, 4],\n",
              "        [6, 6, 8, 9, 8],\n",
              "        [4, 8, 2, 3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텐서 값을 지정해서 생성하는 방법들"
      ],
      "metadata": {
        "id": "MRzpuvnKAozm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [zeros]\n",
        " https://pytorch.org/docs/stable/generated/torch.zeros.html"
      ],
      "metadata": {
        "id": "8r_3chu9_9bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros(3, 3) # torch.zeros(*size) 여기서 size 는 \",\"로 구분하며 차원을 여러개로 늘릴 수 있습니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TfE0OeA_-Ni",
        "outputId": "2244ef74-b213-4ea6-a3c1-49cdf1531516"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [ones]\n",
        "https://pytorch.org/docs/stable/generated/torch.ones.html"
      ],
      "metadata": {
        "id": "gQB0dBOJAEaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones(2, 2, 2) # torch.ones(*size) 여기서 size 는 \",\"로 구분하며 채널을 여러개로 늘릴 수 있습니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qh8mrIWAEAY",
        "outputId": "284e2214-366d-40ce-e877-3fdce3854bef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1.],\n",
              "         [1., 1.]],\n",
              "\n",
              "        [[1., 1.],\n",
              "         [1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [full]\n",
        "https://pytorch.org/docs/stable/generated/torch.full.html"
      ],
      "metadata": {
        "id": "bF3dt9JhATn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.full((2, 3), 5) # torch.full((size),value) => 괄호로 텐서의 크기 (2,3) 를 입력하고, 지정한 값 value (5) 로 모든 요소가 설정됩니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrFROo-XAWEU",
        "outputId": "6b283055-9d8c-46df-ecc4-1c15d5841b41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5, 5, 5],\n",
              "        [5, 5, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [eye]\n",
        "https://pytorch.org/docs/stable/generated/torch.eye.html"
      ],
      "metadata": {
        "id": "2ZELSetNAckT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.eye(3) # torch.eye(n) (nxn) 크기를 가지는 단위 행렬 반환, 단위행렬 특성 상 정사각행렬 (square matrix)만 가능"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5x9HVy7Ahlq",
        "outputId": "8d1a6bd2-1f3c-4989-a2b5-f19a5db66fa6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 다양한 데이터를 텐서 형식으로 변환하기"
      ],
      "metadata": {
        "id": "Ds7Jw3HkEijb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [tensor]\n",
        "https://pytorch.org/docs/stable/generated/torch.tensor.html"
      ],
      "metadata": {
        "id": "dGsIcNN7EmOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* tensor : 주어진 데이터를 텐서로 변환. 데이터는 list, tuple, numpy array 등의 형태일 수 있음."
      ],
      "metadata": {
        "id": "k4k4QWppRL9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list, tuple, numpy array를 텐서로 바꾸기\n",
        "ls = [[1, 2, 3, 4, 5],[6, 7, 8, 9, 10]] # sample list 생성\n",
        "tup = tuple([1, 2, 3]) # sample tuple 생성\n",
        "arr = np.array([[[1, 2, 3],[4, 5, 6]],[[7, 8, 9],[10, 11, 12]]]) # sample numpy array 생성\n",
        "\n",
        "print(torch.tensor(ls))\n",
        "print('\\n')\n",
        "print(torch.tensor(tup))\n",
        "print('\\n')\n",
        "print(torch.tensor(arr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9J0d7BBEvf-",
        "outputId": "469ead30-68e9-413e-c984-4788c078a784"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1,  2,  3,  4,  5],\n",
            "        [ 6,  7,  8,  9, 10]])\n",
            "\n",
            "\n",
            "tensor([1, 2, 3])\n",
            "\n",
            "\n",
            "tensor([[[ 1,  2,  3],\n",
            "         [ 4,  5,  6]],\n",
            "\n",
            "        [[ 7,  8,  9],\n",
            "         [10, 11, 12]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [from_numpy]\n",
        "https://pytorch.org/docs/stable/generated/torch.from_numpy.html"
      ],
      "metadata": {
        "id": "Fuj7D7bWEy11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* from_numpy : numpy array 를 텐서로 변환"
      ],
      "metadata": {
        "id": "AitWqYjiRRc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.from_numpy(arr) # array 를 tensor로 바꾸기 (2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWMKIzBCEztX",
        "outputId": "f3098eb7-c37e-4af8-9fd0-b50c598b3110"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1,  2,  3],\n",
              "         [ 4,  5,  6]],\n",
              "\n",
              "        [[ 7,  8,  9],\n",
              "         [10, 11, 12]]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [as_tensor]\n",
        "https://pytorch.org/docs/stable/generated/torch.as_tensor.html"
      ],
      "metadata": {
        "id": "4uVQzsfKE2yZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* as_tensor: 변환 전 데이터와의 메모리 공유(memory sharing)를 사용하므로, 변환 전 데이터 변경 시 변환되어 있는 텐서에도 반영됨"
      ],
      "metadata": {
        "id": "zimhCsIBRbiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.tensor 와 torch.as_tensor 의 차이점 알아보기\n",
        "print(\"torch.tensor\")\n",
        "data1 = np.array([1, 2, 3, 4, 5]) # 샘플 데이터 리스트 생성\n",
        "tensor1 = torch.tensor(data1) # memory 공유 X\n",
        "data1[0] = 10  # 원본 데이터 변경\n",
        "print(tensor1)  # 원본 데이터의 값 변경에 영향을 받지 않음\n",
        "\n",
        "print('-------'*10)\n",
        "\n",
        "print(\"torch.as_tensor\")\n",
        "data2 = np.array([1, 2, 3, 4, 5])\n",
        "tensor2 = torch.as_tensor(data2) # memory 공유 O\n",
        "data2[0] = 10  # 원본 데이터 변경\n",
        "print(tensor2)  # 원본 데이터의 값 변경에 영향을 받음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7n1O-hZE-zN",
        "outputId": "c108c771-e34a-43a1-d2b4-7da54a3b5e62"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.tensor\n",
            "tensor([1, 2, 3, 4, 5])\n",
            "----------------------------------------------------------------------\n",
            "torch.as_tensor\n",
            "tensor([10,  2,  3,  4,  5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Tensor]\n",
        "https://pytorch.org/docs/stable/tensors.html"
      ],
      "metadata": {
        "id": "LEFJnolNFBjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Tensor : float32 type으로 텐서 변환"
      ],
      "metadata": {
        "id": "Cje4ef7-RhL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1, 2, 3, 4, 5]\n",
        "tensor1 = torch.tensor(data) # list 에서 Tensor 변환\n",
        "print(\"torch.tensor\")\n",
        "print(\"Output:\", tensor1)\n",
        "print(\"Type\", tensor1.dtype) # dtype : Tensor 안의 원소들의 자료형, torch.tensor 는 원본의 데이터 타입을 그대로 따라감\n",
        "\n",
        "\n",
        "print('-------'*3)\n",
        "\n",
        "tensor2 = torch.Tensor(data) # list 에서 Tensor 변환\n",
        "print(\"torch.Tensor\")\n",
        "print(\"Output:\", tensor2)\n",
        "print(\"Type\", tensor2.dtype) # torch.tensor 는 float32 타입으로 Tensor 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l0ZPceiFCVB",
        "outputId": "147c817f-466d-47ca-9d20-9d9754b3dbdd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.tensor\n",
            "Output: tensor([1, 2, 3, 4, 5])\n",
            "Type torch.int64\n",
            "---------------------\n",
            "torch.Tensor\n",
            "Output: tensor([1., 2., 3., 4., 5.])\n",
            "Type torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. 텐서에서의 Indexing 이해 및 실습"
      ],
      "metadata": {
        "id": "9ZkLm7ib-kL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Tensor indexing]\n",
        " https://pytorch.org/cppdocs/notes/tensor_indexing.html"
      ],
      "metadata": {
        "id": "XNvhEtcJGCEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing 은 텐서 내의 특정 **요소**를 index를 통해 접근할 수 있는 방법을 의미합니다.\n",
        "* Indexing 기본 : **대괄호(\"[ ]\")**를 통해 이뤄지며, **\":\"** 는 특정 범위의 접근을 의미.\n"
      ],
      "metadata": {
        "id": "WqPjClvtG2kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1차원 텐서에서 Indexing 하기\n",
        "tmp_1dim = torch.tensor([i for i in range(10)]) # 0부터 9 까지의 값을 가지는 1차원 텐서 생성\n",
        "\n",
        "print(tmp_1dim[0]) # 첫번째 원소 값 추출\n",
        "print(tmp_1dim[5]) # 6번째 원소 값 추출\n",
        "print(tmp_1dim[-1]) # -1 번째 원소 값 (뒤에서 첫번째) 추출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk9ng5oXFGYG",
        "outputId": "5aace005-ad24-4823-bd89-ec186d149d12"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0)\n",
            "tensor(5)\n",
            "tensor(9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3차원 텐서에서 Indexing 하기\n",
        "tmp_3dim = torch.randn(4, 3, 2) # 4채널, 3행, 2열\n",
        "print(\"Shape : \", tmp_3dim.shape)\n",
        "print(tmp_3dim)\n",
        "\n",
        "print('-------'*8)\n",
        "\n",
        "print(tmp_3dim[:,:,0].shape)\n",
        "print(tmp_3dim[:,:,0]) # 전체 채널과 전체 행에서 0번째 열만 추출\n",
        "\n",
        "print('\\n') # 줄 띄움\n",
        "\n",
        "print(tmp_3dim[0,:,1].shape)\n",
        "print(tmp_3dim[0,:,1])  # 0번째 채널의 전체 행에서 1번째 열만 추출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf3WU_kIGVzi",
        "outputId": "565370c3-8016-49a8-a581-3f461c5b4ba9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape :  torch.Size([4, 3, 2])\n",
            "tensor([[[ 4.0570e-01,  1.8425e+00],\n",
            "         [-4.4150e-01,  9.1364e-01],\n",
            "         [-1.3595e+00, -8.9464e-04]],\n",
            "\n",
            "        [[-2.0540e+00,  1.1468e+00],\n",
            "         [ 7.2211e-01,  4.7066e-01],\n",
            "         [ 2.1817e+00, -6.2593e-01]],\n",
            "\n",
            "        [[-6.8956e-01,  5.8297e-02],\n",
            "         [-4.5142e-01,  4.1615e-01],\n",
            "         [ 4.5970e-01, -4.4328e-01]],\n",
            "\n",
            "        [[-1.6927e-01,  1.6617e+00],\n",
            "         [ 9.1560e-01, -5.3600e-01],\n",
            "         [-3.4129e-02,  5.1829e-01]]])\n",
            "--------------------------------------------------------\n",
            "torch.Size([4, 3])\n",
            "tensor([[ 0.4057, -0.4415, -1.3595],\n",
            "        [-2.0540,  0.7221,  2.1817],\n",
            "        [-0.6896, -0.4514,  0.4597],\n",
            "        [-0.1693,  0.9156, -0.0341]])\n",
            "\n",
            "\n",
            "torch.Size([3])\n",
            "tensor([ 1.8425e+00,  9.1364e-01, -8.9464e-04])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [index_select]\n",
        " https://pytorch.org/docs/stable/generated/torch.index_select.html"
      ],
      "metadata": {
        "id": "WmULJhznG5L8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* index_select : 선택한 차원에서 인덱스에 해당하는 요소만 추출하는 함수"
      ],
      "metadata": {
        "id": "xgXBAqA6HCyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# index_select\n",
        "tmp_2dim = torch.tensor([[i for i in range(10)],[i for i in range(10, 20)]])\n",
        "print(tmp_2dim)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "my_index = torch.tensor([0, 2]) # 선택하고자 하는 index 는 텐서 형태이어야 함.\n",
        "torch.index_select(tmp_2dim, dim=1, index=my_index) # 열을 기준으로 0열과 2열을 추출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaMAVWM9HHKp",
        "outputId": "d2a83de3-679d-414a-e284-b2ead8e969b3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  2],\n",
              "        [10, 12]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Masking 을 이용한 Indexing\n",
        "조건에 따른 텐서의 요소를 사용하기 위한 방법으로 조건에 맞는 요소들만 반환하는 방법."
      ],
      "metadata": {
        "id": "H_bhm7S1HNvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mask 를 이용한 텐서 Indexing (조건에 맞는 값만 추출)\n",
        "mask = tmp_2dim >= 5 # 5보다 큰 텐서만 추출\n",
        "tmp_2dim[mask] # 1차원 Tensor 로 반환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAVw_jBHHb2v",
        "outputId": "275fe772-4207-4f6c-8596-7bf8e9a5f90f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [masked_select]\n",
        "https://pytorch.org/docs/stable/generated/torch.masked_select.html"
      ],
      "metadata": {
        "id": "qv0CynmMHSB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* masked_select : 주어진 mask에 해당하는 요소들을 추출하여 1차원으로 펼친 새로운 텐서를 반환하는 함수"
      ],
      "metadata": {
        "id": "8WLkalmjJ_Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.masked_select(tmp_2dim, mask = mask) # tmp_2dim[tmp_2dim >= 5] 와 동일"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skHgH8wtKAT6",
        "outputId": "214f2024-c3c3-413e-c17f-cda93e4b11d6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [take]\n",
        " https://pytorch.org/docs/stable/generated/torch.take.html"
      ],
      "metadata": {
        "id": "1nkRjvmuKGGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* take : 주어진 인덱스를 사용하여 텐서에서 요소를 선택하는 함수. 인덱스 번호는 텐서를 1차원으로 늘려졌을 때 기준으로 접근해야합니다."
      ],
      "metadata": {
        "id": "OTPkdsXBKKAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_2dim = torch.tensor([[i for i in range(10)], [i for i in range(10, 20)]])\n",
        "print(tmp_2dim)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "my_index = torch.tensor([0, 15])\n",
        "torch.take(tmp_2dim, index = my_index) # Tensor가 1차원으로 늘려졌을 때 기준으로 index 번호로 접근"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFuEFKjuKLSU",
        "outputId": "e589bce2-8f3a-47ac-a0f3-fc0cd4e93799"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [gather]\n",
        " https://pytorch.org/docs/stable/generated/torch.gather.html"
      ],
      "metadata": {
        "id": "G7QctUhmKQSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* gather : 주어진 차원에서 인덱스에 해당하는 요소들을 선택하여 새로운 텐서를 반환"
      ],
      "metadata": {
        "id": "6kN-Z1zmKVzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_2dim = torch.tensor([[i for i in range(10)],[i for i in range(10,20)]])\n",
        "print(tmp_2dim)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "recon_index =  torch.tensor([[0 ,1],[9, 8]]) # 0번째 값, 1번 째 값을 0번째 행으로 설정하고, 9번째 값, 8번째 값을 1번째 행으로 설정한다.\n",
        "dim = 1 # 열 기준\n",
        "print(recon_index)\n",
        "print('\\n')\n",
        "\n",
        "torch.gather(tmp_2dim, dim = 1, index = recon_index) # dim =1 이므로 열 기준, 0행 0열, 0행 1열 선택, 1행 9열, 1행 8열"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "229tKOiLKWqT",
        "outputId": "2072f106-c956-4d45-f04b-fd588d9692b5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
            "\n",
            "\n",
            "tensor([[0, 1],\n",
            "        [9, 8]])\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1],\n",
              "        [19, 18]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.텐서의 모양 바꾸기"
      ],
      "metadata": {
        "id": "K0_GDoNW93Wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. 텐서의 shape을 바꾸는 여러가지 함수 이해 및 실습"
      ],
      "metadata": {
        "id": "skLBWV0Y-dp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서에 대한 모양을 변경하기 위해 명심해야 할 점은 텐서의 크기 (요소의 개수)는 유지되어야 한다는 점입니다."
      ],
      "metadata": {
        "id": "vo0EHZMziHZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [size]\n",
        " https://pytorch.org/docs/stable/generated/torch.Tensor.size.html"
      ],
      "metadata": {
        "id": "0VfIee-0h9HO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* size : 텐서의 모양을 확인합니다."
      ],
      "metadata": {
        "id": "3jQRcqJIiEUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(2, 3, 5) # random 한 값을 가진 (2,3,5) 텐서 생성\n",
        "a.size() # 차원 크기 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH-srRQTiJFc",
        "outputId": "e7e3d3a0-2f6d-4f76-ed11-7ac72d45b7b5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape # a.size() 와 동일"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC0KIXZHiULI",
        "outputId": "c3e7cff5-1d21-46e4-d4f1-ab224134d26b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [reshape]\n",
        " https://pytorch.org/docs/stable/generated/torch.reshape.html"
      ],
      "metadata": {
        "id": "u6FsP1gbh32K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* reshape : 텐서의 모양을 변경합니다. 메모리를 공유하지 않습니다."
      ],
      "metadata": {
        "id": "OzFQ1Tesh8DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모양 변경\n",
        "a = torch.randn(2, 3, 5) # (2,3,5) 크기를 가지는 텐서 생성\n",
        "print(a)\n",
        "print(\"Shape : \", a.size()) # 텐서 모양 반환\n",
        "print('\\n')\n",
        "\n",
        "reshape_a = a.reshape(5, 6) # 3차원 텐서를 2차원 텐서로 크기 변경 (2,3,5) -> (5,6)\n",
        "print(reshape_a)\n",
        "print(\"Shape : \", reshape_a.size()) # 변경한 텐서 모양 반환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf52CWOligSB",
        "outputId": "8e1cec07-8080-4eef-aeb6-fd35031702a2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.8293, -0.8782,  0.5931,  0.5846,  1.5093],\n",
            "         [ 0.3523,  0.5225,  1.9458,  1.7045,  0.4754],\n",
            "         [-0.0440, -0.0877,  1.9327, -0.7795, -1.1362]],\n",
            "\n",
            "        [[-1.3770,  0.4299, -0.1022, -0.4668, -0.0831],\n",
            "         [-0.5662,  0.4607,  0.1515, -0.9061,  0.3251],\n",
            "         [-1.0208,  0.4430, -1.3720, -0.9154,  1.7926]]])\n",
            "Shape :  torch.Size([2, 3, 5])\n",
            "\n",
            "\n",
            "tensor([[ 1.8293, -0.8782,  0.5931,  0.5846,  1.5093,  0.3523],\n",
            "        [ 0.5225,  1.9458,  1.7045,  0.4754, -0.0440, -0.0877],\n",
            "        [ 1.9327, -0.7795, -1.1362, -1.3770,  0.4299, -0.1022],\n",
            "        [-0.4668, -0.0831, -0.5662,  0.4607,  0.1515, -0.9061],\n",
            "        [ 0.3251, -1.0208,  0.4430, -1.3720, -0.9154,  1.7926]])\n",
            "Shape :  torch.Size([5, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -1 로 모양 자동 설정\n",
        "# 총 요소수가 30 (2 X 3 X 5) 이므로 n = 10\n",
        "reshape_auto_a = a.reshape(3, -1) # (2,3,5) 크기를 가지는 Tensor를 (3,n)의 모양으로 변경, \"-1\" 로 크기 자동 계산\n",
        "print(reshape_auto_a.size()) # 2x3x5 = 3 x n 의 방정식을 푸는 문제로 n 이 자동설정"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhM4vUjRikh9",
        "outputId": "7414cad0-2989-48c4-e786-78ab7b37d946"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 X n = 30 을 만족하는 정수 n이 존재하지 않음\n",
        "a.reshape(7, -1) #  2x3x5 = 30 = 3 x n 의 방정식의 해가 정수가 아니면 오류 발생"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "eGhRn4HyioBY",
        "outputId": "77459d13-3462-4fda-a9aa-6b2bbd4ff5d6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[7, -1]' is invalid for input of size 30",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-edf96b057d5e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 7 X n = 30 을 만족하는 정수 n이 존재하지 않음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#  2x3x5 = 30 = 3 x n 의 방정식의 해가 정수가 아니면 오류 발생\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[7, -1]' is invalid for input of size 30"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [view]\n",
        " https://pytorch.org/docs/stable/generated/torch.Tensor.view.html"
      ],
      "metadata": {
        "id": "ZHHy1DXphtlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* view : 텐서의 모양을 변경합니다."
      ],
      "metadata": {
        "id": "64ZzJFvnhzt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)\n",
        "print(\"Shape : \", a.size()) # 텐서 모양 반환\n",
        "print('\\n')\n",
        "\n",
        "view_a = a.view(5, 6) # reshape 과 동일하게 (2,3,5) 크기를 (5,6) 크기로 변경\n",
        "print(view_a)\n",
        "print(\"Shape : \", view_a.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9oiWWdUk72C",
        "outputId": "b83bf561-845d-4cad-cd04-59a73560610b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.8293, -0.8782,  0.5931,  0.5846,  1.5093],\n",
            "         [ 0.3523,  0.5225,  1.9458,  1.7045,  0.4754],\n",
            "         [-0.0440, -0.0877,  1.9327, -0.7795, -1.1362]],\n",
            "\n",
            "        [[-1.3770,  0.4299, -0.1022, -0.4668, -0.0831],\n",
            "         [-0.5662,  0.4607,  0.1515, -0.9061,  0.3251],\n",
            "         [-1.0208,  0.4430, -1.3720, -0.9154,  1.7926]]])\n",
            "Shape :  torch.Size([2, 3, 5])\n",
            "\n",
            "\n",
            "tensor([[ 1.8293, -0.8782,  0.5931,  0.5846,  1.5093,  0.3523],\n",
            "        [ 0.5225,  1.9458,  1.7045,  0.4754, -0.0440, -0.0877],\n",
            "        [ 1.9327, -0.7795, -1.1362, -1.3770,  0.4299, -0.1022],\n",
            "        [-0.4668, -0.0831, -0.5662,  0.4607,  0.1515, -0.9061],\n",
            "        [ 0.3251, -1.0208,  0.4430, -1.3720, -0.9154,  1.7926]])\n",
            "Shape :  torch.Size([5, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "view_auto_a = a.view(3, -1) # reshape 과 동일하게 (3,n)의 모양으로 변경. \"-1\" 로 크기 자동 계산\n",
        "print(view_auto_a.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Xkh31YelAO-",
        "outputId": "eebc709c-aa97-4691-b870-f4b7445c3f0b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [transpose]\n",
        "https://pytorch.org/docs/stable/generated/torch.transpose.html"
      ],
      "metadata": {
        "id": "VsVOlq9ZhoWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* transpose : 텐서의 차원을 전치합니다."
      ],
      "metadata": {
        "id": "OSh-JuHkhreV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_a = torch.randint(1, 10, (3, 2, 5)) # 1 ~ 9의 값을 가지는 (3,2,5) 사이즈의 Tensor 생성\n",
        "print(tensor_a)\n",
        "print(\"Shape : \", tensor_a.size())\n",
        "print('\\n')\n",
        "\n",
        "# (3,2,5) 를 (2,3,5) 의 크기로 변경\n",
        "trans_a = tensor_a.transpose(1, 2) # 행과 열을 서로 전치, 서로 전치할 차원 2개를 지정\n",
        "print(trans_a)\n",
        "print(\"Shape : \", trans_a.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW3h7QRKlFKW",
        "outputId": "e0a12ef1-d566-43df-b797-199742a7c935"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[8, 4, 1, 6, 4],\n",
            "         [4, 7, 1, 8, 9]],\n",
            "\n",
            "        [[1, 8, 6, 6, 1],\n",
            "         [1, 5, 5, 2, 6]],\n",
            "\n",
            "        [[7, 2, 2, 3, 2],\n",
            "         [5, 6, 3, 7, 3]]])\n",
            "Shape :  torch.Size([3, 2, 5])\n",
            "\n",
            "\n",
            "tensor([[[8, 4],\n",
            "         [4, 7],\n",
            "         [1, 1],\n",
            "         [6, 8],\n",
            "         [4, 9]],\n",
            "\n",
            "        [[1, 1],\n",
            "         [8, 5],\n",
            "         [6, 5],\n",
            "         [6, 2],\n",
            "         [1, 6]],\n",
            "\n",
            "        [[7, 5],\n",
            "         [2, 6],\n",
            "         [2, 3],\n",
            "         [3, 7],\n",
            "         [2, 3]]])\n",
            "Shape :  torch.Size([3, 5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [permute]\n",
        " https://pytorch.org/docs/stable/generated/torch.permute.html"
      ],
      "metadata": {
        "id": "-ThMVRbAhXcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* permute : 텐서 차원의 순서를 재배열합니다."
      ],
      "metadata": {
        "id": "2rezvwhchigX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor_a)\n",
        "print(\"Shape : \", tensor_a.size())\n",
        "print('\\n')\n",
        "\n",
        "permute_a = tensor_a.permute(0, 2, 1) # (3,2,5)의 모양을 (3,5,2)의 모양으로 변경\n",
        "print(permute_a)\n",
        "print(\"Shape : \", permute_a.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URNabdtplIGb",
        "outputId": "5d238e67-eec2-414b-d753-5ee0512c2e36"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[8, 4, 1, 6, 4],\n",
            "         [4, 7, 1, 8, 9]],\n",
            "\n",
            "        [[1, 8, 6, 6, 1],\n",
            "         [1, 5, 5, 2, 6]],\n",
            "\n",
            "        [[7, 2, 2, 3, 2],\n",
            "         [5, 6, 3, 7, 3]]])\n",
            "Shape :  torch.Size([3, 2, 5])\n",
            "\n",
            "\n",
            "tensor([[[8, 4],\n",
            "         [4, 7],\n",
            "         [1, 1],\n",
            "         [6, 8],\n",
            "         [4, 9]],\n",
            "\n",
            "        [[1, 1],\n",
            "         [8, 5],\n",
            "         [6, 5],\n",
            "         [6, 2],\n",
            "         [1, 6]],\n",
            "\n",
            "        [[7, 5],\n",
            "         [2, 6],\n",
            "         [2, 3],\n",
            "         [3, 7],\n",
            "         [2, 3]]])\n",
            "Shape :  torch.Size([3, 5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. 텐서의 차원을 추가하거나 변경하는 방법에 대한 이해 및 실습"
      ],
      "metadata": {
        "id": "UQ1EP9sT-XA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [unsqueeze]\n",
        " https://pytorch.org/docs/stable/generated/torch.unsqueeze.html"
      ],
      "metadata": {
        "id": "roRBzlQOopfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* unsqueeze : 텐서에 특정 차원에 크기가 1인 차원을 추가."
      ],
      "metadata": {
        "id": "48FBfsTsosGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_a = torch.tensor([i for i in range(10)]).reshape(5, 2) # 0부터 9까지의 숫자들을 (5,2) 크기로 변경\n",
        "print(tensor_a)\n",
        "print('Shape : ', tensor_a.size())\n",
        "print('\\n')\n",
        "\n",
        "unsqu_a = tensor_a.unsqueeze(0) # 0번째 차원 하나 추가 (5,2) => (1,5,2)\n",
        "print(unsqu_a)\n",
        "print('Shape : ', unsqu_a.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOFeqB9wovyc",
        "outputId": "8007e7f3-4c8d-47f9-9587-3dbb0945d5cc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5],\n",
            "        [6, 7],\n",
            "        [8, 9]])\n",
            "Shape :  torch.Size([5, 2])\n",
            "\n",
            "\n",
            "tensor([[[0, 1],\n",
            "         [2, 3],\n",
            "         [4, 5],\n",
            "         [6, 7],\n",
            "         [8, 9]]])\n",
            "Shape :  torch.Size([1, 5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unsqu_a2 = tensor_a.unsqueeze(-1) # 마지막번째에 차원 하나 추가 (5,2) => (5,2,1)\n",
        "print(unsqu_a2)\n",
        "print('Shape : ', unsqu_a2.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBE0Bmt7o7yk",
        "outputId": "422821c6-8063-4473-f039-c3dc47b32c96"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0],\n",
            "         [1]],\n",
            "\n",
            "        [[2],\n",
            "         [3]],\n",
            "\n",
            "        [[4],\n",
            "         [5]],\n",
            "\n",
            "        [[6],\n",
            "         [7]],\n",
            "\n",
            "        [[8],\n",
            "         [9]]])\n",
            "Shape :  torch.Size([5, 2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [squeeze]\n",
        " https://pytorch.org/docs/stable/generated/torch.squeeze.html"
      ],
      "metadata": {
        "id": "nM7ZLdoYodt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* squeeze : 텐서에 차원의 크기가 1인 차원을 제거합니다."
      ],
      "metadata": {
        "id": "mNruzCPvojq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(unsqu_a)\n",
        "print(\"Shape : \", unsqu_a.size())\n",
        "print('\\n')\n",
        "\n",
        "squ = unsqu_a.squeeze() # 차원이 1인 차원을 제거\n",
        "print(squ)\n",
        "print(\"Shape : \", squ.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-pVrSMyo-BU",
        "outputId": "7aa4c9a4-3126-4de1-a572-09f3f6bfa9be"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0, 1],\n",
            "         [2, 3],\n",
            "         [4, 5],\n",
            "         [6, 7],\n",
            "         [8, 9]]])\n",
            "Shape :  torch.Size([1, 5, 2])\n",
            "\n",
            "\n",
            "tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5],\n",
            "        [6, 7],\n",
            "        [8, 9]])\n",
            "Shape :  torch.Size([5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros(2, 1, 2, 1, 2) # 모든 원소가 0인 (2,1,2,1,2) 크기를 가지는 텐서\n",
        "print(\"Shape (original) : \", x.size()) # 원래 텐서 크기\n",
        "print(x)\n",
        "print('\\n')\n",
        "\n",
        "print(\"Shape (squeeze()) :\", x.squeeze().size()) # 차원이 1인 차원이 여러개일 때, 1인 차원 모두 제거\n",
        "print(x)\n",
        "print('\\n')\n",
        "\n",
        "print(\"Shape (squeeze(0)) :\", x.squeeze(0).size()) # 0번째 차원은 차원의 크기가 1이 아니므로, 변화 없음\n",
        "print(x)\n",
        "print('\\n')\n",
        "\n",
        "print(\"Shape (squeeze(1)) :\", x.squeeze(1).size()) # 1번째 차원은 차원의 크기가 1이므로 제거\n",
        "print(x)\n",
        "print('\\n')\n",
        "\n",
        "print(\"Shape (squeeze(0,1,3)) :\", x.squeeze((0, 1, 3)).size()) # 여러 차원 제거 가능 (0번째 차원은 차원의 크기가 1이 아니기 때문에 무시)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPoShq_QpEd3",
        "outputId": "9e914e37-dbb9-44cf-b847-0dee5668f140"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape (original) :  torch.Size([2, 1, 2, 1, 2])\n",
            "tensor([[[[[0., 0.]],\n",
            "\n",
            "          [[0., 0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0., 0.]],\n",
            "\n",
            "          [[0., 0.]]]]])\n",
            "\n",
            "\n",
            "Shape (squeeze()) : torch.Size([2, 2, 2])\n",
            "tensor([[[[[0., 0.]],\n",
            "\n",
            "          [[0., 0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0., 0.]],\n",
            "\n",
            "          [[0., 0.]]]]])\n",
            "\n",
            "\n",
            "Shape (squeeze(0)) : torch.Size([2, 1, 2, 1, 2])\n",
            "tensor([[[[[0., 0.]],\n",
            "\n",
            "          [[0., 0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0., 0.]],\n",
            "\n",
            "          [[0., 0.]]]]])\n",
            "\n",
            "\n",
            "Shape (squeeze(1)) : torch.Size([2, 2, 1, 2])\n",
            "tensor([[[[[0., 0.]],\n",
            "\n",
            "          [[0., 0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0., 0.]],\n",
            "\n",
            "          [[0., 0.]]]]])\n",
            "\n",
            "\n",
            "Shape (squeeze(0,1,3)) : torch.Size([2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [expand]\n",
        "https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html"
      ],
      "metadata": {
        "id": "F8zWwMXNoR2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* expand : 텐서의 값을 반복하여 크기를 확장합니다.\n",
        "  * A 텐서가 1차원일 경우 : A 텐서의 크기가 (m,) 이면 m은 고정하고 (x,m)의 크기로만 확장 가능\n",
        "  * A 텐서가 2차원 이상일 경우 : 크기가 1인 차원에 대해서만 적용 가능. A 텐서의 크기가 (1,m) 이면 (x,m) , (m,1) 이면 (m,y) 로만 확장 가능."
      ],
      "metadata": {
        "id": "94ctoGjTocnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1dim = torch.tensor([1, 2, 3, 4])\n",
        "print(tensor_1dim)\n",
        "print(\"Shape : \", tensor_1dim.size())\n",
        "print('\\n')\n",
        "\n",
        "expand_tensor = tensor_1dim.expand(3, 4) # (,4) 를 (3,4) 의 크기로 확장 (값을 반복)\n",
        "print(expand_tensor)\n",
        "print(\"Shape : \", expand_tensor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hRx9bShpHXQ",
        "outputId": "6aee4104-4a20-40e0-e0e3-43c6c0e5ae61"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4])\n",
            "Shape :  torch.Size([4])\n",
            "\n",
            "\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [1, 2, 3, 4],\n",
            "        [1, 2, 3, 4]])\n",
            "Shape :  torch.Size([3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_2dim = torch.tensor([[1, 2, 3, 4], [1, 2, 3, 4]]) # (2,4) 크기를 가진 Tensor\n",
        "print(tensor_2dim)\n",
        "print(\"Shape : \", tensor_2dim.size())\n",
        "print('\\n')\n",
        "\n",
        "expand_tensor = tensor_2dim.expand(4,4) # (2,4) 를 (4,8) 의 크기로 확장 (값을 반복)\n",
        "print(expand_tensor) # 에러 발생\n",
        "print(\"Shape : \", expand_tensor.size()) # 에러 발생"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "MQ8yWR2epL4T",
        "outputId": "24679269-1dcd-4ad9-faee-c43d227736cf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3, 4],\n",
            "        [1, 2, 3, 4]])\n",
            "Shape :  torch.Size([2, 4])\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The expanded size of the tensor (4) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [4, 4].  Tensor sizes: [2, 4]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ca0aa7093d8c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mexpand_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_2dim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (2,4) 를 (4,8) 의 크기로 확장 (값을 반복)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 에러 발생\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 에러 발생\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (4) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [4, 4].  Tensor sizes: [2, 4]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [repeat]\n",
        "https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html"
      ],
      "metadata": {
        "id": "ytuI-s1zoJ2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* repeat : 텐서를 반복하여 크기를 확장합니다.\n",
        "  * ex) A 텐서가 (m,n) 크기를 가진다하고, A 텐서를 repeat(i,j) 를 하면 결과값으로 (m x i, n x j)의 크기의 텐서가 생성됩니다."
      ],
      "metadata": {
        "id": "C1G-yfFwoQ5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1dim = torch.tensor([1, 2, 3, 4])\n",
        "print(tensor_1dim)\n",
        "print(\"Shape : \", tensor_1dim.size())\n",
        "print('\\n')\n",
        "\n",
        "repeat_tensor = tensor_1dim.repeat(3, 4) # tensor_1dim 자체를 행으로 3번 반복, 열로 4번 반복\n",
        "print(repeat_tensor)\n",
        "print(\"Shape : \", repeat_tensor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0px0cRrpPER",
        "outputId": "e88ef634-da5b-4c71-9564-b9c075811def"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4])\n",
            "Shape :  torch.Size([4])\n",
            "\n",
            "\n",
            "tensor([[1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],\n",
            "        [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],\n",
            "        [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]])\n",
            "Shape :  torch.Size([3, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [flatten]\n",
        " https://pytorch.org/docs/stable/generated/torch.flatten.html"
      ],
      "metadata": {
        "id": "xcTAxdSdn_NY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* flatten : 다차원 텐서를 1차원 텐서로 변경합니다."
      ],
      "metadata": {
        "id": "UvW5o2zGoH6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([i for i in range(20)]).reshape(2, 5, 2) # 0부터 19까지의 숫자를 4행 5열 Tensor로 구성\n",
        "print(t)\n",
        "print(\"Shape : \", t.size())\n",
        "print('\\n')\n",
        "\n",
        "flat_tensor = t.flatten() # (2, 5, 2) 의 Tensor를 (20,)로 모양 변경, 1차원으로 변경\n",
        "print(flat_tensor)\n",
        "print(\"Shape : \", flat_tensor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZiwJ9NCpViS",
        "outputId": "f5d83bfe-2ff9-47c5-e12e-37a2649d686d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0,  1],\n",
            "         [ 2,  3],\n",
            "         [ 4,  5],\n",
            "         [ 6,  7],\n",
            "         [ 8,  9]],\n",
            "\n",
            "        [[10, 11],\n",
            "         [12, 13],\n",
            "         [14, 15],\n",
            "         [16, 17],\n",
            "         [18, 19]]])\n",
            "Shape :  torch.Size([2, 5, 2])\n",
            "\n",
            "\n",
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19])\n",
            "Shape :  torch.Size([20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#start_dim=1 차원 인덱스 1부터 시작하여 그 이후의 모든 차원을 하나의 차원으로 평면화(합치는)하는 것을 의미\n",
        "#start_dim의 기본값은 0이므로, 지정하지 않으면 0부터 모든 차원을 평면화\n",
        "#t의 크기가 (2, 5, 2)일때\n",
        "#start_dim=1로 평탄화한 결과 텐서의 크기는 (2, 12)\n",
        "flat_tensor2 = t.flatten(start_dim=1) # flatten을 시작할 차원을 지정할 수 있음. 지정한 차원 이후의 모든 차원을 하나의 차원으로 평면화, 기본값 = 0 (1차원)\n",
        "print(flat_tensor2)\n",
        "print(flat_tensor2.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIL1IXE3pYRP",
        "outputId": "33102747-5c5a-45ab-86b3-fde5cd971cce"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
            "torch.Size([2, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [ravel]\n",
        " https://pytorch.org/docs/stable/generated/torch.ravel.html"
      ],
      "metadata": {
        "id": "Uv7V405bn4Q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ravel : 다차원 텐서를 1차원 텐서로 변경합니다."
      ],
      "metadata": {
        "id": "eMYXsQ4Zn-JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([i for i in range(20)]).reshape(2, 5, 2) # 0부터 19까지의 숫자를 (2, 5, 2) 크기 Tensor로 구성\n",
        "print(t)\n",
        "print(\"Shape : \", t.size())\n",
        "print('\\n')\n",
        "\n",
        "ravel_tensor = t.ravel() # flatten 과 동일하게 (2,5,2) 의 텐서를 (20,)로 모양 변경, 1차원으로 변경\n",
        "print(ravel_tensor)\n",
        "print(\"Shape : \", ravel_tensor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxgns8azpa66",
        "outputId": "d3f9994f-1c36-44b9-c526-9e3101221108"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0,  1],\n",
            "         [ 2,  3],\n",
            "         [ 4,  5],\n",
            "         [ 6,  7],\n",
            "         [ 8,  9]],\n",
            "\n",
            "        [[10, 11],\n",
            "         [12, 13],\n",
            "         [14, 15],\n",
            "         [16, 17],\n",
            "         [18, 19]]])\n",
            "Shape :  torch.Size([2, 5, 2])\n",
            "\n",
            "\n",
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19])\n",
            "Shape :  torch.Size([20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t.ravel(1) # 에러 발생, ravel 은 flatten 과 달리 어떠한 축을 기준으로 평탄화 하는 작업이 없음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "MwS1lt4Hphr3",
        "outputId": "9cb83d26-5405-4493-cd77-7b87989161ea"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "_TensorBase.ravel() takes no arguments (1 given)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-4ad2f534a783>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 에러 발생, ravel 은 flatten 과 달리 어떠한 축을 기준으로 평탄화 하는 작업이 없음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: _TensorBase.ravel() takes no arguments (1 given)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. 역할이 비슷한 함수들의 차이 이해 및 실습"
      ],
      "metadata": {
        "id": "I_KVbZpF-RRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 모양 변경 : view vs. reshape vs. unsqueeze\n",
        "\n",
        "  * ※ contiguous 란?\n",
        "    * 텐서의 메모리 상에 연속적인 데이터 배치를 갖는 것\n",
        "    * 텐서를 처음 생성 후 정의하면 기본적으로 contiguous 하지만, 이에 대해 차원의 순서를 변경하는 과정을 거치면 contiguous 하지 않습니다.\n",
        "    * 텐서의 contiguous 함을 확인하기 위해선 is_contiguous() 를 사용합니다.\n",
        "  * view 는 contiguous 하지 않은 텐서에 대해서 동작하지 않습니다.\n",
        "  * reshape 는 contiguous 하지 않은 텐서를 contiguous 하게 만들어주고, 크기를 변경합니다.\n",
        "  * unsqueeze 는 차원의 크기가 1인 차원을 추가하지만, 차원의 크기가 1이 아니면 차원의 모양을 변경할 수 없습니다.\n"
      ],
      "metadata": {
        "id": "2GvYQKqYY-YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* [what is contiguous?] : https://titania7777.tistory.com/3\n",
        "* [view vs reshape] :  https://inmoonlight.github.io/2021/03/03/PyTorch-view-transpose-reshape/\n",
        "* [view, reshape, transpose, permute 비교] : https://sanghyu.tistory.com/3"
      ],
      "metadata": {
        "id": "7dmazvaabkb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# view vs reshape\n",
        "tmp = torch.tensor([[[0, 1], [2, 3], [4, 5]], \\\n",
        "                 [[6, 7], [8, 9], [10, 11]], \\\n",
        "                 [[12, 13], [14, 15], [16, 17]], \\\n",
        "                 [[18, 19], [20, 21], [22, 23]]])\n",
        "tmp_t = tmp.transpose(0,1) # contiguous 를 False 로 만들기 위한 작업\n",
        "print(tmp_t.is_contiguous()) # contiguous 한지 검사\n",
        "print(tmp_t.view(-1)) # view는 contiguous 하지 않은 텐서에 대해선 동작이 되지 않음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "jGI2k0IwY-CC",
        "outputId": "ff38cc37-934b-4122-d74c-0708ade83d8d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-58c86dac39d7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtmp_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# contiguous 를 False 로 만들기 위한 작업\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# contiguous 한지 검사\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# view는 contiguous 하지 않은 텐서에 대해선 동작이 되지 않음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reshape_tmp = tmp_t.reshape(-1) # reshape은 contiguous 하지 않아도 동작이 됨\n",
        "print(reshape_tmp)\n",
        "print(reshape_tmp.is_contiguous()) # contiguous 하지 않았던 Tensor를 contiguous 하게 변경해 줌"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef1QOrO_cgYH",
        "outputId": "b707831c-ff6c-42b8-f43b-cd1694422f58"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  1,  6,  7, 12, 13, 18, 19,  2,  3,  8,  9, 14, 15, 20, 21,  4,  5,\n",
            "        10, 11, 16, 17, 22, 23])\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (view , reshape) vs unsqueeze\n",
        "tensor_a = torch.randn(2, 3)\n",
        "# (2, 3) 의 텐서를 (2, 3, 1)의 크기로 변경\n",
        "view_tensor = tensor_a.view(2, 3, 1) # view 를 이용하여 (2,3,1) 의 크기로 변경\n",
        "reshape_tensor = tensor_a.reshape(2, 3, 1) # reshape 를 이용하여 (2,3,1) 의 크기로 변경\n",
        "unsqueeze_tensor = tensor_a.unsqueeze(-1) # unsqueeze 를 이용하여 (2,3,1) 의 크기로 변경\n",
        "\n",
        "print(\"View output size : \",view_tensor.size())\n",
        "print(\"Reshape output size : \",reshape_tensor.size())\n",
        "print(\"Unsqueeze output size : \",unsqueeze_tensor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wf7uaygci0A",
        "outputId": "787fcf57-4120-480b-95e8-482a2cfc922c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View output size :  torch.Size([2, 3, 1])\n",
            "Reshape output size :  torch.Size([2, 3, 1])\n",
            "Unsqueeze output size :  torch.Size([2, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 차원 변경 : transpose vs. permute\n",
        "  * transpose : 두 차원에 대해서만 변경이 가능\n",
        "    * 인자가 총 2개여야함.\n",
        "  * permute : 모든 차원에 대해서 변경이 가능\n",
        "    * 인자가 차원의 개수와 동일해야 함."
      ],
      "metadata": {
        "id": "AIYmabARYyp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [view, reshape, transpose, permute 비교] :  https://sanghyu.tistory.com/3"
      ],
      "metadata": {
        "id": "EpdHrgKkYnRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "tensor_a = torch.randn(2, 3, 2)\n",
        "transpose_tensor = tensor_a.transpose(2, 1) # 행과 열을 전치\n",
        "permute_tensor = tensor_a.permute(0, 2, 1) # 행과 열을 바꿈.\n",
        "\n",
        "print(\"Transpose tensor shape : \", transpose_tensor.size())\n",
        "print(\"Permute tensor shape : \", permute_tensor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jQzOFZdYmum",
        "outputId": "b9697c60-9969-46f9-e404-84f06b156527"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transpose tensor shape :  torch.Size([2, 2, 3])\n",
            "Permute tensor shape :  torch.Size([2, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 반복을 통한 텐서 크기 확장 : expand vs. repeat\n",
        "  * expand\n",
        "    * 원본 텐서와 메모리를 공유한다.\n",
        "  * repeat\n",
        "    * 원본 텐서와 메모리를 공유하지 않는다."
      ],
      "metadata": {
        "id": "INgVwU_UYjHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [expand vs repeat] :  https://seducinghyeok.tistory.com/9"
      ],
      "metadata": {
        "id": "BPNsUBPLYSyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 원본 텐서 생성\n",
        "tensor_a = torch.rand(1, 1, 3)\n",
        "print('Original Tensor Size')\n",
        "print(tensor_a.size())\n",
        "print(tensor_a)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# expand 사용하여 (1,1,3) => (4, 1, 3)\n",
        "expand_tensor = tensor_a.expand(4, 1, -1)\n",
        "print(\"Shape of expanded tensor:\", expand_tensor.size())\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# repeat 사용하여 (1,1,3) => (4, 1, 3)\n",
        "repeat_tensor = tensor_a.repeat(4, 1, 1)\n",
        "print(\"Shape of repeated tensor:\", repeat_tensor.size())\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# 평면화된 뷰 수정 후 원본 텐서 확인\n",
        "tensor_a[:] = 0\n",
        "\n",
        "print(\"Expanded Tensor\")\n",
        "print(expand_tensor) # 값 변경이 됨\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(\"Repeated Tensor\")\n",
        "print(repeat_tensor) # 깂 변경 안됨"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzgU7UwHYmBm",
        "outputId": "a6f2d506-adad-4771-80cc-7ea9f9632f19"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tensor Size\n",
            "torch.Size([1, 1, 3])\n",
            "tensor([[[0.9630, 0.2828, 0.1282]]])\n",
            "\n",
            "\n",
            "Shape of expanded tensor: torch.Size([4, 1, 3])\n",
            "\n",
            "\n",
            "Shape of repeated tensor: torch.Size([4, 1, 3])\n",
            "\n",
            "\n",
            "Expanded Tensor\n",
            "tensor([[[0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.]]])\n",
            "\n",
            "\n",
            "Repeated Tensor\n",
            "tensor([[[0.9630, 0.2828, 0.1282]],\n",
            "\n",
            "        [[0.9630, 0.2828, 0.1282]],\n",
            "\n",
            "        [[0.9630, 0.2828, 0.1282]],\n",
            "\n",
            "        [[0.9630, 0.2828, 0.1282]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.텐서 합치기 나누기"
      ],
      "metadata": {
        "id": "x-V9suWs97zP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. 여러 텐서를 합치기"
      ],
      "metadata": {
        "id": "8EzBxfNo-Fan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [cat]\n",
        " https://pytorch.org/docs/stable/generated/torch.cat.html"
      ],
      "metadata": {
        "id": "PYxyAQM4gZHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* cat : 주어진 차원을 따라 텐서들을 연결합니다. (주어진 차원 외의 다른 차원의 크기가 같아야합니다.)"
      ],
      "metadata": {
        "id": "Lhqm2qZRgfrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_a = torch.randint(1, 10, (2, 3)) # 1부터 9까지의 무작위 정수가 있는 (2,3) Tensor\n",
        "tensor_b = torch.rand(5, 3) # 0부터 1까지의 균등분포를 따르는 (5,3) Tensor\n",
        "\n",
        "print(\"Tensor A shape : \", tensor_a.size())\n",
        "print(tensor_a)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(\"Tensor B shape : \", tensor_b.size())\n",
        "print(tensor_b)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "a_cat_b_row = torch.cat((tensor_a, tensor_b), dim=0) # dim = 0 (행), Tensor A 와 Tensor B 를 행 기준으로 합친다.\n",
        "print(\"Concat Tensor A and B (by row) Shape : \", a_cat_b_row.shape) # (Tensor A 행 개수 + Tensor B 행 개수, Tensor A/B 열 개수)\n",
        "print(a_cat_b_row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5nG_MxKgl37",
        "outputId": "a4838e2b-f748-4752-ad9f-e742e93f8836"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor A shape :  torch.Size([2, 3])\n",
            "tensor([[3, 1, 9],\n",
            "        [9, 1, 4]])\n",
            "\n",
            "\n",
            "Tensor B shape :  torch.Size([5, 3])\n",
            "tensor([[0.5903, 0.3840, 0.0322],\n",
            "        [0.0140, 0.8915, 0.4381],\n",
            "        [0.6267, 0.1685, 0.0406],\n",
            "        [0.0211, 0.0702, 0.1205],\n",
            "        [0.0900, 0.2382, 0.9741]])\n",
            "\n",
            "\n",
            "Concat Tensor A and B (by row) Shape :  torch.Size([7, 3])\n",
            "tensor([[3.0000, 1.0000, 9.0000],\n",
            "        [9.0000, 1.0000, 4.0000],\n",
            "        [0.5903, 0.3840, 0.0322],\n",
            "        [0.0140, 0.8915, 0.4381],\n",
            "        [0.6267, 0.1685, 0.0406],\n",
            "        [0.0211, 0.0702, 0.1205],\n",
            "        [0.0900, 0.2382, 0.9741]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [stack]\n",
        " https://pytorch.org/docs/stable/generated/torch.stack.html"
      ],
      "metadata": {
        "id": "VtYErmKjgmbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* stack : 주어진 차원을 새로운 차원으로 추가하여 텐서들을 쌓습니다.\n",
        "  * 합쳐질 텐서들의 크기는 모두 같아야합니다."
      ],
      "metadata": {
        "id": "yG2v2ydRgqq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_a = torch.randint(1, 10, (3, 2))  # 1부터 9까지의 무작위 정수가 있는 (3,2) Tensor\n",
        "tensor_b = torch.rand(3, 2)  # 0부터 1까지의 균등분포를 따르는 (3,2) Tensor\n",
        "\n",
        "print(\"Tensor A shape : \", tensor_a.size())\n",
        "print(tensor_a)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(\"Tensor B shape : \", tensor_b.size())\n",
        "print(tensor_b)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "stack_tensor_row = torch.stack([tensor_a, tensor_b], dim=0)  # dim = 0, 행을 기준으로 Tensor A 에 Tensor B 를 쌓기\n",
        "print(\"Stack A and B (by row): \", stack_tensor_row.size()) # (쌓은 Tensor 개수, Tensor A/B 행 개수, Tensor A/B 열 개수)\n",
        "print(stack_tensor_row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsLZ37C1gsQq",
        "outputId": "fbfe8671-9ffc-426a-c616-4fbf96278102"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor A shape :  torch.Size([3, 2])\n",
            "tensor([[8, 5],\n",
            "        [4, 5],\n",
            "        [7, 7]])\n",
            "\n",
            "\n",
            "Tensor B shape :  torch.Size([3, 2])\n",
            "tensor([[0.1038, 0.1068],\n",
            "        [0.3240, 0.8726],\n",
            "        [0.8022, 0.2432]])\n",
            "\n",
            "\n",
            "Stack A and B (by row):  torch.Size([2, 3, 2])\n",
            "tensor([[[8.0000, 5.0000],\n",
            "         [4.0000, 5.0000],\n",
            "         [7.0000, 7.0000]],\n",
            "\n",
            "        [[0.1038, 0.1068],\n",
            "         [0.3240, 0.8726],\n",
            "         [0.8022, 0.2432]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. 하나의 텐서를 여러 텐서로 나누기"
      ],
      "metadata": {
        "id": "JBqOt734-KmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [chunk]\n",
        " https://pytorch.org/docs/stable/generated/torch.chunk.html"
      ],
      "metadata": {
        "id": "U3a-2QPSg0JK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* chunk : 나누고자 하는 **텐서의 개수**를 지정하여 원래의 텐서를 개수에 맞게 분리합니다.\n",
        "  * chunks 인자\n",
        "    * 몇 **개**의 텐서로 나눌 것인지"
      ],
      "metadata": {
        "id": "A4UB5n3lg5M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_a = torch.randint(1, 10, (6, 4))  # (6,4) 텐서\n",
        "print(\"Original : \", tensor_a)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "chunk_num = 3\n",
        "chunk_tensor = torch.chunk(tensor_a, chunks = chunk_num, dim=0)  # dim = 0 (행), 6개의 행이 3개로 나누어 떨어지므로 3개의 텐서로 분리\n",
        "print(f'{len(chunk_tensor)} 개의 Tensor로 분리')\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "for idx,a in enumerate(chunk_tensor):\n",
        "    print(f'{idx} 번째 Tensor \\n{a}')\n",
        "    print(f'{idx} 번째 Tensor 크기', a.size())\n",
        "    print('---'*10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7NXTKyvg6W4",
        "outputId": "98b997a9-8035-4b87-c5f9-9cbfadc9d08a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original :  tensor([[3, 2, 6, 1],\n",
            "        [6, 7, 3, 2],\n",
            "        [7, 6, 1, 9],\n",
            "        [1, 5, 9, 5],\n",
            "        [2, 6, 6, 1],\n",
            "        [8, 5, 9, 5]])\n",
            "\n",
            "\n",
            "3 개의 Tensor로 분리\n",
            "\n",
            "\n",
            "0 번째 Tensor \n",
            "tensor([[3, 2, 6, 1],\n",
            "        [6, 7, 3, 2]])\n",
            "0 번째 Tensor 크기 torch.Size([2, 4])\n",
            "------------------------------\n",
            "1 번째 Tensor \n",
            "tensor([[7, 6, 1, 9],\n",
            "        [1, 5, 9, 5]])\n",
            "1 번째 Tensor 크기 torch.Size([2, 4])\n",
            "------------------------------\n",
            "2 번째 Tensor \n",
            "tensor([[2, 6, 6, 1],\n",
            "        [8, 5, 9, 5]])\n",
            "2 번째 Tensor 크기 torch.Size([2, 4])\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [split]\n",
        " https://pytorch.org/docs/stable/generated/torch.split.html"
      ],
      "metadata": {
        "id": "3m3ASYz5hEDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* split : 입력한 **크기**로 여러 개의 작은 텐서로 나눕니다.\n",
        "  * split_size_or_sections 인자\n",
        "    * split_size (int): 얼마만큼의 크기로 자를 것인지\n",
        "    * sections (list): 얼마만큼의 크기로 **각각** 자를 것인지 (리스트 형태로 각 텐서의 크기를 각각 지정해 줄 수 있음)"
      ],
      "metadata": {
        "id": "gl7fCIrShBXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_a = torch.randint(1, 10, (6, 4))  # (6,4) 텐서\n",
        "print(tensor_a)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "split_size = 2\n",
        "split_tensor = torch.split(tensor_a , split_size_or_sections = split_size, dim=0)  # dim = 0 (행), 텐서 A 를 행의 길이가 2 (split_size)인 텐서로 나눔\n",
        "print(f'{len(split_tensor)} 개의 Tensor로 분리')\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "for idx,a in enumerate(split_tensor):\n",
        "    print(f'{idx} 번째 Tensor \\n{a}')\n",
        "    print(f'{idx} 번째 Tensor 크기', a.size())\n",
        "    print('---'*10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wog9z_HYhFMW",
        "outputId": "dcaf67ea-e8a3-43a2-e840-3ef740bffa43"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2, 4, 6, 4],\n",
            "        [7, 4, 9, 7],\n",
            "        [9, 6, 3, 5],\n",
            "        [7, 5, 5, 2],\n",
            "        [9, 7, 6, 1],\n",
            "        [2, 2, 5, 8]])\n",
            "\n",
            "\n",
            "3 개의 Tensor로 분리\n",
            "\n",
            "\n",
            "0 번째 Tensor \n",
            "tensor([[2, 4, 6, 4],\n",
            "        [7, 4, 9, 7]])\n",
            "0 번째 Tensor 크기 torch.Size([2, 4])\n",
            "------------------------------\n",
            "1 번째 Tensor \n",
            "tensor([[9, 6, 3, 5],\n",
            "        [7, 5, 5, 2]])\n",
            "1 번째 Tensor 크기 torch.Size([2, 4])\n",
            "------------------------------\n",
            "2 번째 Tensor \n",
            "tensor([[9, 7, 6, 1],\n",
            "        [2, 2, 5, 8]])\n",
            "2 번째 Tensor 크기 torch.Size([2, 4])\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_a = torch.randint(1, 10, (6, 4))  # (6,4) 텐서\n",
        "print(\"Original : \", tensor_a)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "split_num = [2, 4]\n",
        "split_tensor = torch.split(tensor_a, split_size_or_sections = split_num, dim=0)  # dim = 0 (행), 텐서 A 를 행의 길이가 (2개인 텐서와 4개인 텐서)로 나눔\n",
        "print(f'{len(split_tensor)} 개의 Tensor로 분리')\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "for idx,a in enumerate(split_tensor):\n",
        "    print(f'{idx} 번째 Tensor \\n{a}')\n",
        "    print(f'{idx} 번째 Tensor 크기', a.size())\n",
        "    print('---'*10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNE3wNSKhc6x",
        "outputId": "c371b0cd-0310-4d1b-9e41-0cb372c52416"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original :  tensor([[3, 9, 5, 3],\n",
            "        [9, 7, 2, 3],\n",
            "        [5, 3, 7, 7],\n",
            "        [3, 8, 2, 9],\n",
            "        [9, 2, 4, 1],\n",
            "        [1, 7, 8, 4]])\n",
            "\n",
            "\n",
            "2 개의 Tensor로 분리\n",
            "\n",
            "\n",
            "0 번째 Tensor \n",
            "tensor([[3, 9, 5, 3],\n",
            "        [9, 7, 2, 3]])\n",
            "0 번째 Tensor 크기 torch.Size([2, 4])\n",
            "------------------------------\n",
            "1 번째 Tensor \n",
            "tensor([[5, 3, 7, 7],\n",
            "        [3, 8, 2, 9],\n",
            "        [9, 2, 4, 1],\n",
            "        [1, 7, 8, 4]])\n",
            "1 번째 Tensor 크기 torch.Size([4, 4])\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ]
}