{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzASid3v6bUa"
      },
      "source": [
        "# **STEP 14. 웹 데이터 수집**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYKVkAY06gaz"
      },
      "source": [
        "## **1. 웹 크롤링 개념 다지기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Puf35u3pAbHM"
      },
      "source": [
        "## **2. BeautifulSoup 기본 개념**\n",
        "\n",
        "*   BeautifulSoup 패키지 추가\n",
        "*   패키지가 없는 경우, 설치 명령어 : pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWIiepQCAotK"
      },
      "source": [
        "# BeautifulSoup 불러오기\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvUa62rGJprU"
      },
      "source": [
        "## **3. BeautifulSoup HTML 코드 작성**\n",
        "- html이라는 변수에 간단한 html 코드 저장\n",
        "- html 코드는 \"\"\" 사이에 입력\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wZM78ocyJfW"
      },
      "source": [
        "# HTML 코드 작성\n",
        "html =  \"\"\"\n",
        "<html>\n",
        "\t<body>\n",
        "\t<h1 id = 'title'>Selena 파이썬 라이브러리 활용!</h1>\n",
        "\t<p id = 'body'>오늘의 주제는 웹 데이터 수집</p>\n",
        "\t<p class = 'scraping'>삼성전자 일별 시세 불러오기</p>\n",
        "\t<p class = 'scraping'>이해 쏙쏙 Selena 수업!</p>\n",
        "\t</body>\n",
        "<html>\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTIn8UH3KBtn"
      },
      "source": [
        "## **4. BeautifulSoup HTML 파싱**\n",
        "- html을 파이썬에서 읽을 수 있게 파싱(파이썬 객체로 변환)\n",
        "- html이라는 변수에 저장한 html 소스코드를 .parser를 붙여 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6VNNWXDyJTN"
      },
      "source": [
        "# BeautifulSoup 함수를 이용하여 soup 객체 생성\n",
        "# html이라는 변수에 저장한 html 소스코드를 .parser를 붙여 변환\n",
        "soup = BeautifulSoup(html, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzyQL9eO6vq1"
      },
      "source": [
        "## **5. BeautifulSoup 데이터를 텍스트로 반환**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSMazsfeKjbj"
      },
      "source": [
        "### **5-1. soup**\n",
        "- soup : soup의 데이터를 모두 가져와서 텍스트로 반환\n",
        "- soup.contents : soup의 데이터를 모두 가져와서 리스트로 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLodTGfFyJP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad6b439-0dd7-4d2e-e384-041bfae42e76"
      },
      "source": [
        "# soup의 데이터를 모두 가져와서 텍스트로 반환\n",
        "for text in soup:\n",
        "  print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "<html>\n",
            "<body>\n",
            "<h1 id=\"title\">Selena 파이썬 라이브러리 활용!</h1>\n",
            "<p id=\"body\">오늘의 주제는 웹 데이터 수집</p>\n",
            "<p class=\"scraping\">삼성전자 일별 시세 불러오기</p>\n",
            "<p class=\"scraping\">이해 쏙쏙 Selena 수업!</p>\n",
            "</body>\n",
            "<html>\n",
            "</html></html>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h2LWGIAKxZh"
      },
      "source": [
        "### **5-2. soup.stripped_strings**\n",
        "- soup.stripped_strings : 공백도 함께 제거하여 텍스트로 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4HqXf3iydUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce01f33-b19f-49d2-c97a-516753d2ae9f"
      },
      "source": [
        "# soup의 데이터를 모두 가져와서\n",
        "# 공백도 함께 제거하여 텍스트로 반환\n",
        "for stripped_text in soup.stripped_strings:\n",
        "  print(stripped_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selena 파이썬 라이브러리 활용!\n",
            "오늘의 주제는 웹 데이터 수집\n",
            "삼성전자 일별 시세 불러오기\n",
            "이해 쏙쏙 Selena 수업!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nejJdunq643E"
      },
      "source": [
        "## **6. BeautifulSoup Find 함수**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74HnK4JDLTxf"
      },
      "source": [
        " ### **6-1. find()**\n",
        "- find 함수는 id, class, element 등을 검색 가능\n",
        "- find : 조건에 해당하는 첫 번째 정보만 검색\n",
        "> - 클래스 이름을 알 경우, class_ 형태로 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRpNnk8a5xDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e37ac2-9ca3-4ab6-eda9-eb5702ddd68d"
      },
      "source": [
        "# id 값이 'title'인 조건에 해당하는 첫 번째 정보만 검색\n",
        "title = soup.find(id='title')\n",
        "print(title )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<h1 id=\"title\">Selena 파이썬 라이브러리 활용!</h1>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tapo3CS56G-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e33247-aeb6-4b45-d062-b0872a92f4b8"
      },
      "source": [
        "# class 값이 'scraping'인 조건에 해당하는 첫 번째 정보만 검색\n",
        "# 클래스 이름을 알 경우, class_ 형태로 사용\n",
        "scraping = soup.find(class_='scraping')\n",
        "print(scraping )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<p class=\"scraping\">삼성전자 일별 시세 불러오기</p>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPxYuS2qLkwW"
      },
      "source": [
        "### **6-2. find_all()**\n",
        " - find_all : 조건에 해당하는 모든 정보 검색"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPO_Iudp6hsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c17ac1-dc0e-4cf3-8363-6fc5795e681d"
      },
      "source": [
        "# class 값이 'scraping'인 조건에 해당하는 모든 정보 검색\n",
        "\n",
        "scraping_all = soup.find_all(class_='scraping')\n",
        "print(scraping_all )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<p class=\"scraping\">삼성전자 일별 시세 불러오기</p>, <p class=\"scraping\">이해 쏙쏙 Selena 수업!</p>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_R9JmPZLp3S"
      },
      "source": [
        "### **6-3. string**\n",
        "- string : 태그 내부의 텍스트만 출력\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlGoYUqN-k9n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ce6358f3-f09c-4920-d2ea-dd9f32be7624"
      },
      "source": [
        "# 태그 내부의 텍스트만 출력\n",
        "scraping.string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'삼성전자 일별 시세 불러오기'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz3FNycvMdCV"
      },
      "source": [
        "## **7. BeautifulSoup 웹 크롤링 3단계 과정**\n",
        "- Request : 웹 페이지의 URL 이용해서 HTML 문서를 요청\n",
        "> - 첫 번째 단계인 Request를 위해 import urllib.request 불러오기\n",
        "- Response : 요청한 HTML 문서를 회신\n",
        "- Parsing : 태그 기반으로 파싱(일련의 문자열을 의미 있는 단위로 분해)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VU8o3qSA3uJ"
      },
      "source": [
        "# 웹 페이지의 URL을 이용해서 HTML 문서를 요청하기 위해 필요한 라이브러리\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV8lnnn8_zJ2"
      },
      "source": [
        "## **8. BeautifulSoup F12(개발자 도구) URL 찾기**\n",
        "- 1) 네이버 금융 홈페이지 접속 : https://finance.naver.com/\n",
        "- 2) 삼성전자(code : 005930) 검색\n",
        "- 3) 시세 메뉴 클릭 후 URL 확인 : https://finance.naver.com/item/sise.naver?code=005930\n",
        "- 4) 키보드 F12(개발자 도구) 클릭 > 메뉴 Elements 클릭 > 키보드 Ctrl과 F (검색 단축기) 클릭 > '일별 시세' 검색 > scr 값 복사  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIakImtQ7QWR"
      },
      "source": [
        "## **9. BeautifulSoup 첫 번째 단계. Request**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91Th9jk0vl2L"
      },
      "source": [
        "### **9-1. URL 저장**\n",
        "- stock_url이라는 변수에 네이버 금융 사이트의 삼성전자 시세 정보가 담긴 URL 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4sMCjdzNNy9"
      },
      "source": [
        "# stock_url이라는 변수에 URL 저장\n",
        "stock_url = 'https://finance.naver.com/item/sise.naver?code=005930'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZa44ymTwA0_"
      },
      "source": [
        "### **9-2. User-agent 설정**\n",
        "- user-agent 확인 사이트 : http://www.useragentstring.com/\n",
        "- user-agent란, 웹 크롤링을 진행하면 종종 페이지에서 아무것도 받아오지 못하는 경우 발생! 이유는 대부분 서버에서 봇을 차단하기 때문\n",
        "- 그래서 user-agent를 headers에 저장하면 오류를 해결할 수 있음\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzosBm6rNNwh"
      },
      "source": [
        "# header에 user-agent 값 저장\n",
        "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxkokBIIvL5A"
      },
      "source": [
        "### **9-3. requests.get()**\n",
        "- 웹 페이지의 URL 이용해서 HTML 문서를 요청\n",
        "- requests.get(stock_url, headers = headers)\n",
        "> - URL 값을 파라미터 값으로 입력\n",
        "> - 해당 사이트는 반드시 헤더 정보를 요구하기 때문에 파라미터 값으로 헤더 입력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1o179F-YMl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6677444-ed72-48f5-aa85-599c9863e925"
      },
      "source": [
        "# requests.get 함수를 통해 URL를 이용하여 HTML 문서를 요청\n",
        "requests.get(stock_url, headers = headers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0xiDK8Owoab"
      },
      "source": [
        "## **10. BeautifulSoup 두 번째 단계. Response**\n",
        "- 요청한 HTML 문서를 회신\n",
        "- response = requests.get(stock_url, headers = headers)\n",
        "> - 서버에서 요청을 받아 처리한 후, 요청자에게 응답 줌\n",
        "> - HTML 코드 형태"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doKWLYgvv6GG"
      },
      "source": [
        "# response 변수에 요청한 HTML 문서를 회신하여 저장\n",
        "response = requests.get(stock_url, headers = headers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Visw_c57xfaG"
      },
      "source": [
        "## **11. BeautifulSoup 세 번째 단계. Parsing**\n",
        "- 태그 기반으로 파싱(일련의 문자열을 의미 있는 단위로 분해)\n",
        "- soup = BeautifulSoup(response.text, 'html.parser')\n",
        "> - html을 파이썬에서 읽을 수 있게 파싱(파이썬 객체로 변환)\n",
        "> - Response.text에 저장한 html 소스코드를 .parser를 붙여 변환\n",
        "> - parser는 파이썬의 내장 메소드\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTxswDpsT7NL"
      },
      "source": [
        "# soup 변수에 BeautifulSoup의 객체 생성\n",
        "# HTML 코드를 파이썬에서 읽을 수 있도록 파싱\n",
        "soup = BeautifulSoup(response.text, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz-qnkLQBg2j"
      },
      "source": [
        "## **12. BeautifulSoup 반복문으로 일별 종가 구현**\n",
        "- 1) 200일 일별 종가 정보는 1 Page 당 10일의 일별 종가 정보 담겨있어서 20 Page 필요\n",
        "- 2) 일별 종가 담긴 URL과 Header 정보로 requests.get 함수 구현\n",
        "- 3) 요청한 HTML 문서를 회신하여 response 변수에 저장\n",
        "- 4) BeautifulSoup함수로 HTML을 읽을 수 있도록 파싱하여 soup 변수에 저장\n",
        "- 5) Page 개수만큼 20번 반복\n",
        "> - \"tr\" 태그 조건에 해당하는 모든 정보를 검색하여 parsing_list 변수에 저장\n",
        "- 6) 1 Page 당 10일의 일별 종가 정보 담겨있어서 10번 반복\n",
        "> - \"td\" 태그의 align가 \"center\"인 값들 중 0번째 조건에 해당하는 정보 검색하여 출력\n",
        "> - \"td\" 태그의 class가 \"num\"인 값들 중 0번째 조건에 해당하는 정보 검색하여 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Oiy7xRJM9-"
      },
      "source": [
        "**태그 정보는 F12(개발자 도구) 클릭하여 찾기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4-YJETfn0Ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b1d475e-70bc-4d2e-868b-3f48129dd227"
      },
      "source": [
        "# 200일 동안의 일별 종가 정보 가져오는 반복문(1페이지 당 10일 정보 담겨있음)\n",
        "for page in range(1, 21):\n",
        "  print (str(page))\n",
        "\n",
        "  # url + page 번호 합치기\n",
        "  stock_url = 'http://finance.naver.com/item/sise_day.nhn?code=005930' +'&page='+ str(page)\n",
        "\n",
        "  # header 정보\n",
        "  headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
        "\n",
        "  # request : 웹 페이지의 URL, header 이용해서 HTML 문서 요청\n",
        "  # response : 요청한 HTML 문서 회신\n",
        "  response = requests.get(stock_url, headers = headers)\n",
        "\n",
        "  # parsing : HTML을 읽을 수 있도록 파싱\n",
        "  # soup 변수에 BeautifulSoup의 객체 생성\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  # \"tr\" 태그 조건에 해당하는 모든 정보 검색\n",
        "  parsing_list = soup.find_all(\"tr\")\n",
        "\n",
        "  # None 값은 걸러주기 위한 변수 생성\n",
        "  isCheckNone = None\n",
        "\n",
        "  # 페이지당 일별 종가 출력하기 위한 반복문 <들여쓰기 주의>\n",
        "  for i in range(1, len(parsing_list)):\n",
        "    # None 값은 걸러주기 위한 조건문 <들여쓰기 주의>\n",
        "    # .span()는 매치된 문자열의 (시작, 끝)에 해당하는 튜플을 돌려주는 함수\n",
        "    if(parsing_list[i].span != isCheckNone):\n",
        "      # parsing_list[i] : i번째 parsing_list, i 번째 \"tr\" 태그 값\n",
        "      # .find_all(\"td\", align=\"center\")[0].text : \"td\" 태그의 align가 \"center\"인 값들 중 0번째 값\n",
        "      # .find_all(\"td\", class_=\"num\")[0].text : \"td\" 태그의 class가 \"num\"인 값들 중 0번째 값\n",
        "      print(parsing_list[i].find_all(\"td\", align=\"center\")[0].text,\n",
        "            parsing_list[i].find_all(\"td\", class_=\"num\")[0].text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2024.04.26 76,700\n",
            "2024.04.25 76,300\n",
            "2024.04.24 78,600\n",
            "2024.04.23 75,500\n",
            "2024.04.22 76,100\n",
            "2024.04.19 77,600\n",
            "2024.04.18 79,600\n",
            "2024.04.17 78,900\n",
            "2024.04.16 80,000\n",
            "2024.04.15 82,200\n",
            "2\n",
            "2024.04.12 83,700\n",
            "2024.04.11 84,100\n",
            "2024.04.09 83,600\n",
            "2024.04.08 84,500\n",
            "2024.04.05 84,500\n",
            "2024.04.04 85,300\n",
            "2024.04.03 84,100\n",
            "2024.04.02 85,000\n",
            "2024.04.01 82,000\n",
            "2024.03.29 82,400\n",
            "3\n",
            "2024.03.28 80,800\n",
            "2024.03.27 79,800\n",
            "2024.03.26 79,900\n",
            "2024.03.25 78,200\n",
            "2024.03.22 78,900\n",
            "2024.03.21 79,300\n",
            "2024.03.20 76,900\n",
            "2024.03.19 72,800\n",
            "2024.03.18 72,800\n",
            "2024.03.15 72,300\n",
            "4\n",
            "2024.03.14 74,300\n",
            "2024.03.13 74,100\n",
            "2024.03.12 73,300\n",
            "2024.03.11 72,400\n",
            "2024.03.08 73,300\n",
            "2024.03.07 72,200\n",
            "2024.03.06 72,900\n",
            "2024.03.05 73,700\n",
            "2024.03.04 74,900\n",
            "2024.02.29 73,400\n",
            "5\n",
            "2024.02.28 73,200\n",
            "2024.02.27 72,900\n",
            "2024.02.26 72,800\n",
            "2024.02.23 72,900\n",
            "2024.02.22 73,100\n",
            "2024.02.21 73,000\n",
            "2024.02.20 73,300\n",
            "2024.02.19 73,800\n",
            "2024.02.16 72,800\n",
            "2024.02.15 73,000\n",
            "6\n",
            "2024.02.14 74,000\n",
            "2024.02.13 75,200\n",
            "2024.02.08 74,100\n",
            "2024.02.07 75,000\n",
            "2024.02.06 74,400\n",
            "2024.02.05 74,300\n",
            "2024.02.02 75,200\n",
            "2024.02.01 73,600\n",
            "2024.01.31 72,700\n",
            "2024.01.30 74,300\n",
            "7\n",
            "2024.01.29 74,400\n",
            "2024.01.26 73,400\n",
            "2024.01.25 74,100\n",
            "2024.01.24 74,000\n",
            "2024.01.23 75,200\n",
            "2024.01.22 75,100\n",
            "2024.01.19 74,700\n",
            "2024.01.18 71,700\n",
            "2024.01.17 71,000\n",
            "2024.01.16 72,600\n",
            "8\n",
            "2024.01.15 73,900\n",
            "2024.01.12 73,100\n",
            "2024.01.11 73,200\n",
            "2024.01.10 73,600\n",
            "2024.01.09 74,700\n",
            "2024.01.08 76,500\n",
            "2024.01.05 76,600\n",
            "2024.01.04 76,600\n",
            "2024.01.03 77,000\n",
            "2024.01.02 79,600\n",
            "9\n",
            "2023.12.28 78,500\n",
            "2023.12.27 78,000\n",
            "2023.12.26 76,600\n",
            "2023.12.22 75,900\n",
            "2023.12.21 75,000\n",
            "2023.12.20 74,800\n",
            "2023.12.19 73,400\n",
            "2023.12.18 72,900\n",
            "2023.12.15 73,300\n",
            "2023.12.14 73,100\n",
            "10\n",
            "2023.12.13 72,800\n",
            "2023.12.12 73,500\n",
            "2023.12.11 73,000\n",
            "2023.12.08 72,600\n",
            "2023.12.07 71,500\n",
            "2023.12.06 71,700\n",
            "2023.12.05 71,200\n",
            "2023.12.04 72,600\n",
            "2023.12.01 72,000\n",
            "2023.11.30 72,800\n",
            "11\n",
            "2023.11.29 72,700\n",
            "2023.11.28 72,700\n",
            "2023.11.27 71,300\n",
            "2023.11.24 71,700\n",
            "2023.11.23 72,400\n",
            "2023.11.22 72,800\n",
            "2023.11.21 72,800\n",
            "2023.11.20 72,700\n",
            "2023.11.17 72,500\n",
            "2023.11.16 72,800\n",
            "12\n",
            "2023.11.15 72,200\n",
            "2023.11.14 70,800\n",
            "2023.11.13 70,400\n",
            "2023.11.10 70,500\n",
            "2023.11.09 70,300\n",
            "2023.11.08 69,900\n",
            "2023.11.07 70,900\n",
            "2023.11.06 70,900\n",
            "2023.11.03 69,600\n",
            "2023.11.02 69,700\n",
            "13\n",
            "2023.11.01 68,600\n",
            "2023.10.31 66,900\n",
            "2023.10.30 67,300\n",
            "2023.10.27 67,300\n",
            "2023.10.26 66,700\n",
            "2023.10.25 68,000\n",
            "2023.10.24 68,500\n",
            "2023.10.23 68,400\n",
            "2023.10.20 68,800\n",
            "2023.10.19 69,500\n",
            "14\n",
            "2023.10.18 70,500\n",
            "2023.10.17 69,400\n",
            "2023.10.16 67,300\n",
            "2023.10.13 68,000\n",
            "2023.10.12 68,900\n",
            "2023.10.11 68,200\n",
            "2023.10.10 66,400\n",
            "2023.10.06 66,000\n",
            "2023.10.05 66,700\n",
            "2023.10.04 67,500\n",
            "15\n",
            "2023.09.27 68,400\n",
            "2023.09.26 68,600\n",
            "2023.09.25 69,400\n",
            "2023.09.22 68,800\n",
            "2023.09.21 68,900\n",
            "2023.09.20 69,600\n",
            "2023.09.19 69,800\n",
            "2023.09.18 70,200\n",
            "2023.09.15 72,000\n",
            "2023.09.14 71,700\n",
            "16\n",
            "2023.09.13 70,900\n",
            "2023.09.12 70,500\n",
            "2023.09.11 70,800\n",
            "2023.09.08 70,300\n",
            "2023.09.07 70,400\n",
            "2023.09.06 70,000\n",
            "2023.09.05 70,700\n",
            "2023.09.04 71,200\n",
            "2023.09.01 71,000\n",
            "2023.08.31 66,900\n",
            "17\n",
            "2023.08.30 67,100\n",
            "2023.08.29 66,800\n",
            "2023.08.28 66,800\n",
            "2023.08.25 67,100\n",
            "2023.08.24 68,200\n",
            "2023.08.23 67,100\n",
            "2023.08.22 66,600\n",
            "2023.08.21 66,600\n",
            "2023.08.18 66,300\n",
            "2023.08.17 66,700\n",
            "18\n",
            "2023.08.16 67,000\n",
            "2023.08.14 67,300\n",
            "2023.08.11 67,500\n",
            "2023.08.10 68,000\n",
            "2023.08.09 68,900\n",
            "2023.08.08 67,600\n",
            "2023.08.07 68,500\n",
            "2023.08.04 68,300\n",
            "2023.08.03 68,800\n",
            "2023.08.02 69,900\n",
            "19\n",
            "2023.08.01 71,100\n",
            "2023.07.31 69,800\n",
            "2023.07.28 70,600\n",
            "2023.07.27 71,700\n",
            "2023.07.26 69,800\n",
            "2023.07.25 70,000\n",
            "2023.07.24 70,400\n",
            "2023.07.21 70,300\n",
            "2023.07.20 71,000\n",
            "2023.07.19 71,700\n",
            "20\n",
            "2023.07.18 72,000\n",
            "2023.07.17 73,300\n",
            "2023.07.14 73,400\n",
            "2023.07.13 71,900\n",
            "2023.07.12 71,900\n",
            "2023.07.11 71,500\n",
            "2023.07.10 69,500\n",
            "2023.07.07 69,900\n",
            "2023.07.06 71,600\n",
            "2023.07.05 72,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by9wQrKtMol6"
      },
      "source": [
        "## **13. Pandas 일별 시세 테이블 구현**\n",
        "- 1) Pandas 라이브러리와 Requests 라이브러리 이용\n",
        "- 2) 200일 일별 종가 정보는 1 Page 당 10일의 일별 종가 정보 담겨있어서 20 Page 필요\n",
        "- 3) 일별 종가 담긴 URL과 Header 정보로 requests.get 함수 구현\n",
        "- 4) pandas.read_html 함수를 통해 HTML 불러와서 파싱\n",
        "- 5) append 함수를 이용하여 dataframe 끝에 추가하고 싶은 요소를 추가하여 dataframe 리턴\n",
        "- 6) dropna 함수를 통해 결측 값 제거\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brb_XqwbxHEb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "2955e83d-479b-4eb4-c6cd-c75fc66efa64"
      },
      "source": [
        "# 네이버 금융 일별 시세 테이블 불러오기\n",
        "# pandas 라이브러리와 requests 라이브러리 이용\n",
        "# code = 회사 코드, page = 일별 시세 테이블의 페이지 수 (200 행의 데이터 불러오려면 20 페이지 입력)\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for page in range(1,21):\n",
        "    stock_url = 'http://finance.naver.com/item/sise_day.nhn?code=005930' +'&page='+ str(page)\n",
        "\n",
        "    # header 정보\n",
        "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
        "\n",
        "    # request : 웹 페이지의 URL, header 이용해서 HTML 문서 요청\n",
        "    # response : 요청한 HTML 문서 회신\n",
        "    response = requests.get(stock_url, headers = headers)\n",
        "\n",
        "    # response.text로 응답을 주면 HTML 코드이기 때문에 read_html로 불러오기\n",
        "    # append() : dataframe 끝에 추가하고 싶은 요소를 추가하여 새로운 dataframe을 리턴\n",
        "    df = df.append(pd.read_html(response.text, header=0)[0], ignore_index=True)\n",
        "\n",
        "# 결측 값 있는 행 제거\n",
        "df = df.dropna()\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'append'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-63a04cc383bf>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# response.text로 응답을 주면 HTML 코드이기 때문에 read_html로 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# append() : dataframe 끝에 추가하고 싶은 요소를 추가하여 새로운 dataframe을 리턴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# 결측 값 있는 행 제거\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **자기 주도 학습 복습 과제 - 문제**\n",
        "* 문제를 풀고 답을 확인하세요 :)\n",
        "* 정답지에 자세히 설명되어 있습니다.\n",
        "* [정답지 링크](https://drive.google.com/file/d/1kPQiu8sY47lfA-ObSejr-gqoV6Wq4i8H/view?usp=sharing)"
      ],
      "metadata": {
        "id": "I7Yz1QIbXgc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. BeautifulSoup 문제 (1)**\n",
        "\n",
        "* BeautifulSoup 선언\n",
        "\n"
      ],
      "metadata": {
        "id": "-a6JKY2wvV9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) BeautifulSoup 불러오기\n"
      ],
      "metadata": {
        "id": "9yJ5HJ4OvW3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. BeautifulSoup 문제 (2)**\n",
        "\n",
        "* BeautifulSoup SK하이닉스 시간별 시세 테이블 가져오기\n",
        "- [1] 네이버 금융 홈페이지 접속 : https://finance.naver.com/\n",
        "- [2] SK하이닉스(code : 000660) 검색\n",
        "- [3)] 시세 메뉴 클릭 후 URL 확인 : https://finance.naver.com/item/sise.naver?code=000660\n",
        "- [4] 키보드 F12(개발자 도구) 클릭 > 메뉴 Elements 클릭 > 키보드 Ctrl과 F (검색 단축기) 클릭 > '시간별' 검색 > scr 값 복사 : /item/sise_time.naver?code=000660&thistime=20220929130246  \n",
        "- [1]과 [4] 링크 합쳐서 URL로 저장\n",
        "\n"
      ],
      "metadata": {
        "id": "E_QpPYTxvYrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) stock_url이라는 변수에 Sk하이닉스 시간별 시세의 scr를 URL 저장\n",
        "# [1] 네이버 금융 홈페이지 접속 : https://finance.naver.com/\n",
        "# [4] 키보드 F12(개발자 도구) 클릭 > 메뉴 Elements 클릭 > 키보드 Ctrl과 F (검색 단축기) 클릭 > '시간별' 검색 > scr 값 복사 : /item/sise_time.naver?code=000660&thistime=20220929130246\n",
        "# [1]과 [4] 링크 합쳐서 URL로 저장\n",
        "stock_url = \"           \""
      ],
      "metadata": {
        "id": "T2MVdkbrvZy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. BeautifulSoup 문제 (3)**\n",
        "\n",
        "* BeautifulSoup User-agent 설정\n",
        "- user-agent 확인 사이트 : http://www.useragentstring.com/\n",
        "\n"
      ],
      "metadata": {
        "id": "gmv67Ertvu_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) header에 user-agent 값 저장\n",
        "headers = {'User-Agent':'                  '}"
      ],
      "metadata": {
        "id": "V9nMwNO_vwM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. BeautifulSoup 문제 (4)**\n",
        "\n",
        "* BeautifulSoup 첫 번째 단계. Request"
      ],
      "metadata": {
        "id": "F7T-qv4tv1uF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) requests 불러오기\n"
      ],
      "metadata": {
        "id": "WzavU_Wev42f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) stock_url, headers 정보를 이용하여 HTML 문서를 요청\n"
      ],
      "metadata": {
        "id": "v1gus628v6uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. BeautifulSoup 문제 (5)**\n",
        "\n",
        "* BeautifulSoup 두 번째 단계. Response"
      ],
      "metadata": {
        "id": "Etn2DRVGv8pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) 요청한 HTML 문서를 회신하여 response 변수에 저장\n"
      ],
      "metadata": {
        "id": "RCIv-5Zsv9a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. BeautifulSoup 문제 (6)**\n",
        "\n",
        "* BeautifulSoup 세 번째 단계. Parsing"
      ],
      "metadata": {
        "id": "gIfebAUQv_P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) soup 변수에 BeautifulSoup의 객체 생성하고 HTML 코드를 파이썬에서 읽을 수 있도록 파싱\n"
      ],
      "metadata": {
        "id": "me2QWyoEwAEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. BeautifulSoup 문제 (7)**\n",
        "\n",
        "* BeautifulSoup 데이터를 텍스트로 반환"
      ],
      "metadata": {
        "id": "KbmhZa18wCHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) soup 변수에서 'tr' 태그 조건에 해당하는 모든 정보를 검색하여 parsing_list변수에 저장\n"
      ],
      "metadata": {
        "id": "SjwV2htqwCsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. BeautifulSoup 문제 (8)**\n",
        "\n",
        "* BeautifulSoup 데이터 출력을 위한 for문"
      ],
      "metadata": {
        "id": "sF60ancBwFIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) 수업 복습으로 for문 한 줄씩 읽으면서 이해하기\n",
        "\n",
        "# None 값은 걸러주기 위한 변수 생성\n",
        "isCheckNone = None\n",
        "\n",
        "# 페이지당 일별 종가 출력하기 위한 반복문 <들여쓰기 주의>\n",
        "for i in range(1, len(parsing_list)):\n",
        "  # None 값은 걸러주기 위한 조건문 <들여쓰기 주의>\n",
        "  # .span()는 매치된 문자열의 (시작, 끝)에 해당하는 튜플을 돌려주는 함수\n",
        "  if(parsing_list[i].span != isCheckNone):\n",
        "    # parsing_list[i] : i번째 parsing_list, i 번째 \"tr\" 태그 값\n",
        "    # .find_all(\"td\", align=\"center\")[0].text : \"td\" 태그의 align가 \"center\"인 값들 중 0번째 값\n",
        "    # .find_all(\"td\", class_=\"num\")[0].text : \"td\" 태그의 class가 \"num\"인 값들 중 0번째 값\n",
        "    print(parsing_list[i].find_all(\"td\", align=\"center\")[0].text,\n",
        "          parsing_list[i].find_all(\"td\", class_=\"num\")[0].text)"
      ],
      "metadata": {
        "id": "uYLI-A7RwFsb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}